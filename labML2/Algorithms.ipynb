{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бронников Максим Андреевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*М8О-307Б-17, №4 по списку*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max120199@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> Алгоритмы </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общие функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # for grafic\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    TN = np.logical_not(Y_val | Y_pred).sum()\n",
    "    return (TP + TN) / len(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    return TP / Y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(Y_val, Y_pred):\n",
    "    TP = (Y_val * Y_pred).sum()\n",
    "    return TP / Y_val.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_metric(Y_val, Y_pred):\n",
    "    precision = Precision(Y_val, Y_pred)\n",
    "    recall = Recall(Y_val, Y_pred)\n",
    "    return 2.0 * recall * precision / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Логистическая регрессия* - метод, при котором ищутся веса $w$ путём минимизации логистической функции потерь: $w = \\underset{w}{\\operatorname{argmin}}(\\sum\\limits_{i = 1}^{n} \\log{(1 + e^{-y_i \\langle w, x_i \\rangle})} ).$\n",
    "\n",
    "Тогда классифицирующий алгоритм определяется: $a(x) = sign(\\langle w, x \\rangle)$, где вектор $x$ содержит в начале константный еденичный элемент, а множество ответов классификатора: $Y = \\{+1, -1\\}.$\n",
    "\n",
    "При этом метод корректно оценивает вероятности отнесения к положительному классу при помощи сигмоид-функции: $p(y = +1|x) = \\displaystyle\\frac{1}{1 + e^{-\\langle w, x \\rangle}}.$\n",
    "\n",
    "Для обучения модели можно использовать алгоритм *Градиентного спуска* в силу дифференцируемости функционала ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулязационные нормы для градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_norm(vector):\n",
    "    return (vector**2).sum()\n",
    "\n",
    "def L1_norm(vector):\n",
    "    return np.abs(vec).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты от норм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_grad(vector):\n",
    "    return vector\n",
    "\n",
    "def L1_grad(vector):\n",
    "    return vector / np.abs(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решатель на основе градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self, speed, gradient_func, regulasator=None, \n",
    "                 C=10.0, eps = 0.001, maxsteps=250):\n",
    "        self.speed = speed\n",
    "        self.function = gradient_func\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "        if regulasator == \"l1\":\n",
    "            self.regulasator = lambda w:  L1_grad(w) / C\n",
    "        elif regulasator == \"l2\":\n",
    "            self.regulasator = lambda w: L2_grad(w) / C\n",
    "        else:\n",
    "            self.regulasator = lambda w: 0.0\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        # init w0\n",
    "        w0 = np.zeros(X_train.shape[1])\n",
    "        w = np.random.random(X_train.shape[1])\n",
    "        k = 1\n",
    "        while np.linalg.norm(w - w0) > self.eps and k <= self.maxsteps:\n",
    "            w0 = w\n",
    "            temp = self.speed * ((1 / k)**0.5) # like vowpal step temp\n",
    "            w = w - temp*(self.function(X_train, Y_train, w) + self.regulasator(w))\n",
    "            k += 1\n",
    "            \n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор функций, которые непосредственно относятся к логистической регрессии: сигмоида, функция потерь и её градиент по вектору весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this exponent numericall stable\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "def logit_loss(wx, y_real):\n",
    "    return np.log(1.0 + np.exp(-wx*y_real)).sum()\n",
    "\n",
    "def logit_grad(x, y, w):\n",
    "    koeff = (y * sigmoid(-y*x.dot(w)))\n",
    "    koeff = koeff.reshape((koeff.shape[0], 1)) # make a column\n",
    "    return -(koeff * x).sum(axis = 0) # full gradient - sum of gradients on ever x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель регрессии с использованием градиентного решателя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with 2 classes: 0 and 1.\n",
    "class BinaryLogisticRegression:\n",
    "    # main params\n",
    "    def __init__(self, speed = 1.5, reg_type=None, C=2.0, eps=0.001, maxsteps=200):\n",
    "        # init solver\n",
    "        self.solver = GradientDescent(speed, logit_grad, reg_type, C, eps, maxsteps)\n",
    "        # init weight  variable\n",
    "        self.w = None\n",
    "        \n",
    "    # training\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # convert 0 to -1 for algo\n",
    "        Y = np.array(Y_train)\n",
    "        Y[Y_train == 0] = -1\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_train.shape[0], 1))\n",
    "        X = np.hstack((x0, X_train))\n",
    "        # train weight by gradient descent\n",
    "        self.w = self.solver.fit(X, Y)\n",
    "        return self\n",
    "    \n",
    "    # returns predictes classes\n",
    "    def predict(self, X_val, border = 0):\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_val.shape[0], 1))\n",
    "        X = np.hstack((x0, X_val))\n",
    "        # <w, x> product for all examples\n",
    "        Xw = X.dot(self.w)\n",
    "        # make predict: 0 - negative, 1 - positive\n",
    "        Y_pred = np.zeros(Xw.shape).astype(np.int8)\n",
    "        # a(x) = [<w,x> > t], t - border\n",
    "        Y_pred[Xw >= border] = 1\n",
    "        return Y_pred\n",
    "    \n",
    "    # probs of positive class\n",
    "    def predict_proba(self, X_val):\n",
    "        # add np.ones colomn for w0 weight:\n",
    "        x0 = np.ones((X_val.shape[0], 1))\n",
    "        X = np.hstack((x0, X_val))\n",
    "        # <w, x> product for all examples\n",
    "        Xw = X.dot(self.w)\n",
    "        # return proba\n",
    "        return sigmoid(Xw)\n",
    "    \n",
    "    # compute metrics\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.w\n",
    "\n",
    "    # for fun\n",
    "    def __str__(self):\n",
    "        return \"Logistic Regression model with gradient descent!\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Logistic Regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод опорных векторов ищет вектор множетелей Лагранжа: $\\lambda = \\{\\lambda_1, ..., \\lambda_n \\}: \\; \\lambda = \\underset{\\lambda}{\\operatorname{argmax}}(\\sum\\limits_{i = 1}^{n} \\lambda_i - \\displaystyle\\frac{1}{2}\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n} \\lambda_i \\lambda_j y_i y_j K(x_i, x_j)).$  Порог $w_o$ расчитывется в процессе итераций.\n",
    "\n",
    "\n",
    "В линейном случае $K(x_i, x_j) = \\langle x_i, x_j \\rangle.$ При этом веса $(w, w_0)$ модели можно найти следующим образом: $w = \\sum\\limits_{i=1}^{n}\\lambda_i y_i x_i$, $w_0 = \\underset{i: \\; 0 < \\lambda_i < C}{\\operatorname{median}}(\\langle w, x_i \\rangle - y_i).$\n",
    "\n",
    "Эта задача решается эффективно при помощи алгоритма **S**equental **M**inimal **O**ptimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequental Minimal Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_SMO:\n",
    "    def __init__(self, X, Y, kernel, C, eps, maxsteps, linear):\n",
    "        self.K = kernel\n",
    "        self.C = C\n",
    "        self.tol = eps\n",
    "        self.maxsteps = maxsteps\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.linear = linear\n",
    "        self.l = np.zeros(Y.shape)\n",
    "        self.e_cache = -Y.astype(np.float64) # zero prediction - y\n",
    "        #self.n = len(X)\n",
    "        self.w0 = 0.0\n",
    "        if linear:\n",
    "            self.w = np.zeros(X.shape[1])\n",
    "\n",
    "\n",
    "    def KKT_violated(self, idx):\n",
    "        r =  self.Y[idx] * self.e_cache[idx]\n",
    "        l = self.l[idx]\n",
    "        # if (M < 1 - tol & l < C) || (M > 1 + tol & l > 0) => KKT violated\n",
    "        # tol - accuracy of computing\n",
    "        return (l < self.C and r < -self.tol) or (l > 0.0 and r > self.tol)\n",
    "\n",
    "    # search by all element ones\n",
    "    def first_heuristic_one(self):\n",
    "        for idx in range(self.l.shape[0]):\n",
    "            if self.KKT_violated(idx):\n",
    "                yield idx\n",
    "\n",
    "    # search by all non-bound elements \n",
    "    def first_heuristic_two(self):\n",
    "        # i think here loop faster then pre-choice by numpy\n",
    "        # because this loop more effective\n",
    "        for idx in range(self.l.shape[0]):\n",
    "            if 0.0 < self.l[idx] < self.C:\n",
    "                if self.KKT_violated(idx):\n",
    "                    yield idx\n",
    "\n",
    "    # search elem that maximaze |E1 - E2| from non-boundary\n",
    "    def second_heuristic_one(self, idx):\n",
    "        # search |E1 - E2| of each elem\n",
    "        dE = np.abs(self.e_cache - self.e_cache[idx])\n",
    "        # all boundary elems not interesting\n",
    "        dE[(self.l >= self.C) | (self.l <= 0.0)] = 0.0\n",
    "        # return i2 = argmax|E1 - E2|\n",
    "        return np.argmax(dE)\n",
    "\n",
    "    def second_heuristic_two(self, idx):\n",
    "        mask = (self.l < self.C) & (self.l > 0.0)\n",
    "        mask[idx] = False\n",
    "        idxes = np.nonzero(mask)[0]\n",
    "        order = np.random.permutation(len(idxes))\n",
    "        return idxes[order]\n",
    "\n",
    "\n",
    "    def second_heuristic_three(self, idx):\n",
    "        #if second heuristic without result, get idxs without elems from second\n",
    "        # it will faster\n",
    "        mask = (self.l >= self.C) | (self.l <= 0.0)\n",
    "        mask[idx] = False\n",
    "        idxes = np.nonzero(mask)[0]\n",
    "        order = np.random.permutation(len(idxes))\n",
    "        return idxes[order]\n",
    "\n",
    "    def get_weights(self):\n",
    "        if self.linear:\n",
    "            return self.w\n",
    "\n",
    "    def get_support(self):\n",
    "        # returns only support vctors with params if you \n",
    "        # dont get them after train\n",
    "        mask = self.l > 0.0\n",
    "        return self.X[mask], self.Y[mask], self.l[mask], self.w0\n",
    "\n",
    "    # return lambdas\n",
    "    def get_coeffs(self):\n",
    "        return self.l\n",
    "\n",
    "    \n",
    "    def optimize_two(self, i1, i2):\n",
    "        # it emulates machine e in computations\n",
    "        eps = 0.00000000000001\n",
    "\n",
    "        y1 = self.Y[i1]\n",
    "        y2 = self.Y[i2]\n",
    "        x1 = self.X[i1]\n",
    "        x2 = self.X[i2]\n",
    "        l1 = self.l[i1]\n",
    "        l2 = self.l[i2]\n",
    "        E1 = self.e_cache[i1]\n",
    "        E2 = self.e_cache[i2]\n",
    "\n",
    "        # compute L H\n",
    "        if y1 == y2:\n",
    "            L = max(0.0, l2 + l1 - self.C)\n",
    "            H = min(self.C, l2 + l1)\n",
    "        else:\n",
    "            L = max(0.0, l2 - l1)\n",
    "            H = min(self.C, self.C + l2 - l1)\n",
    "        if L == H:\n",
    "            return False\n",
    "\n",
    "        # eta\n",
    "        nu = self.K(x1, x1) + self.K(x2, x2) - 2.0*self.K(x1, x2)\n",
    "\n",
    "        # compute l2\n",
    "        if nu > 0.0:\n",
    "            l2 += y2 * (E1 - E2) / nu\n",
    "            if l2 < L:\n",
    "                l2 = L\n",
    "            elif l2 > H:\n",
    "                l2 = H\n",
    "        else:\n",
    "            c1 = nu/2\n",
    "            c2 = y2 * (E1 - E2) + nu * l2\n",
    "            Lobj = c2*L - c1*L*L\n",
    "            Hobj = c2*H - c1*H*H\n",
    "            if Lobj > Hobj + eps:\n",
    "                l2 = L\n",
    "            elif Lobj < Hobj - eps:\n",
    "                l2 = H\n",
    "\n",
    "        if np.abs(l2 - self.l[i2]) < eps*(l2 + self.l[i2] + eps):\n",
    "            return False\n",
    "\n",
    "        # compute l1\n",
    "        l1 -= y1*y2*(l2 - self.l[i2])\n",
    "        \n",
    "        if l1 < 0.0:\n",
    "            l2 += y1*y2*l1\n",
    "            l1 = 0.0\n",
    "        elif l1 > self.C:\n",
    "            l2 += y1*y2*(l1 - self.C)\n",
    "            l1 = self.C\n",
    "\n",
    "        # update w0:\n",
    "        b1 = self.w0 + E1 + y1*(l1 - self.l[i1])*self.K(x1, x1) + y2*(l2 - self.l[i2])*self.K(x1, x2)\n",
    "        b2 = self.w0 + E2 + y1*(l1 - self.l[i1])*self.K(x1, x2) + y2*(l2 - self.l[i2])*self.K(x2, x2)\n",
    "        if l1 > 0.0 and l1 < self.C:\n",
    "            bnew = b1\n",
    "        elif l2 > 0.0 and l2 < self.C:\n",
    "            bnew = b2\n",
    "        else:\n",
    "            bnew = (b1 + b2) / 2.0\n",
    "        \n",
    "        dw0 = bnew - self.w0\n",
    "        self.w0 = bnew\n",
    "\n",
    "        # update E_cache\n",
    "        t1 = y1*(l1 - self.l[i1])\n",
    "        t2 = y2*(l2 - self.l[i2])\n",
    "        \n",
    "        self.e_cache += t1*self.K(self.X, x1) + t2*self.K(self.X, x2) - dw0\n",
    "        \n",
    "        #update w:\n",
    "        if self.linear:\n",
    "            self.w += t1*x1 + t2*x2\n",
    "        # update lambdas:\n",
    "        self.l[i1] = l1\n",
    "        self.l[i2] = l2\n",
    "\n",
    "        return True\n",
    "            \n",
    "            \n",
    "\n",
    "    # search 2 lambdas for optimize and change them\n",
    "    def train(self):\n",
    "        steps = 0\n",
    "        non_bound_loop = True\n",
    "        # main loop\n",
    "        while steps < self.maxsteps:\n",
    "            non_bound_loop ^= True\n",
    "            # outer loops searches i1:\n",
    "            if not non_bound_loop:\n",
    "                # h1-1\n",
    "                changed = False\n",
    "                for idx1 in self.first_heuristic_one():\n",
    "                    # h2-1\n",
    "                    idx2 = self.second_heuristic_one(idx1)\n",
    "                    if self.optimize_two(idx1, idx2):\n",
    "                        changed = True\n",
    "                        continue\n",
    "                    # h2-2,3 together if h2-1 not worked\n",
    "                    # concat idxs of heuristics in sequence\n",
    "                    extra_heuristics = np.concatenate([\n",
    "                        self.second_heuristic_two(idx1),\n",
    "                        self.second_heuristic_three(idx1)\n",
    "                    ])\n",
    "\n",
    "                    for idx2 in extra_heuristics:\n",
    "                        if self.optimize_two(idx1, idx2):\n",
    "                            changed = True\n",
    "                            break\n",
    "                steps += 1\n",
    "                # if nothing changed - work done\n",
    "                if not changed:\n",
    "                    break\n",
    "            else:\n",
    "                # h1-2\n",
    "                # while changing itterate non-bound elements\n",
    "                while changed and steps < self.maxsteps:\n",
    "                    changed = False\n",
    "                    # itterate non-bound elements\n",
    "                    for idx1 in self.first_heuristic_two():\n",
    "                        # h2-1\n",
    "                        idx2 = self.second_heuristic_one(idx1)\n",
    "                        if self.optimize_two(idx1, idx2):\n",
    "                            changed = True\n",
    "                            continue\n",
    "                        # h2-2,3 together if h2-1 not worked\n",
    "                        # concat idxs of heuristics in sequence\n",
    "                        extra_heuristics = np.concatenate([\n",
    "                            self.second_heuristic_two(idx1),\n",
    "                            self.second_heuristic_three(idx1)\n",
    "                        ])\n",
    "                        for idx2 in extra_heuristics:\n",
    "                            if self.optimize_two(idx1, idx2):\n",
    "                                changed = True\n",
    "                                break\n",
    "                    steps += 1\n",
    "        # after train return support vectors with lambdas and Y\n",
    "        mask = self.l > 0.0\n",
    "        return self.X[mask], self.Y[mask], self.l[mask], self.w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры ядер для метода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return x1.dot(x2.T)\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma):\n",
    "    if len(x1.shape) == 1:\n",
    "        x1r = x1.reshape((1, x1.shape[0]))\n",
    "    else:\n",
    "        x1r = x1\n",
    "    if len(x2.shape) == 1:\n",
    "        x2r = x2.reshape((1, x2.shape[0]))\n",
    "    else:\n",
    "        x2r = x2\n",
    "    ans = np.zeros((x1r.shape[0], x2r.shape[0]))\n",
    "    for i in np.arange(x1r.shape[0]):\n",
    "        # \n",
    "        ans[i] = np.exp(-gamma  * ((x2r - x1r[i])**2).T.sum(axis = 0))\n",
    "    if len(x1.shape) == 1:\n",
    "        ans = ans[0]\n",
    "    if len(x2.shape) == 1:\n",
    "        ans = ans.T[0]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация метода опорных векторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay kernel None for faster linaear version\n",
    "class BinarySVM:\n",
    "    def __init__(self, kernel = None, C=1.0, eps = 0.001, maxsteps=1000):\n",
    "        self.C = C\n",
    "        self.linear = False\n",
    "        if kernel is None:\n",
    "            kernel = lambda x1, x2: x1.dot(x2.T)\n",
    "            self.linear = True\n",
    "        self.K = kernel\n",
    "        self.w = None\n",
    "        self.w0 = None\n",
    "        self.l = None\n",
    "        self.svX = None\n",
    "        self.svY = None\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "    \n",
    "    def weights(self):\n",
    "        if self.linear:\n",
    "            return np.append(self.w0, self.w)\n",
    "        \n",
    "    def predict(self, X_val, border = 0):\n",
    "        # make predict: 0 - negative, 1 - positive\n",
    "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
    "        if self.linear:\n",
    "            x0 = np.ones((X_val.shape[0], 1))\n",
    "            X = np.hstack((x0, X_val))\n",
    "            # <w, x> product for all examples\n",
    "            Xw = X.dot(np.append(-self.w0, self.w))\n",
    "            # a(x) = [<w,x> > t], t - border\n",
    "            Y_pred[Xw >= border] = 1\n",
    "            return Y_pred\n",
    "        else:\n",
    "            yl = (self.svY * self.l).reshape((self.svY.shape[0], 1))\n",
    "            U = self.K(yl * self.svX, X_val).sum(axis = 0) - self.w0\n",
    "            Y_pred[U >= border] = 1\n",
    "            return Y_pred\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        X = X_train\n",
    "        Y = np.array(Y_train)\n",
    "        Y[Y_train == 0] = -1\n",
    "        smo = Binary_SMO(X, Y, self.K, self.C, self.eps, self.maxsteps, self.linear)\n",
    "        self.svX, self.svY, self.l, self.w0 = smo.train()\n",
    "        if self.linear:\n",
    "            self.w = smo.get_weights()\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Support Vector Machine\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SVM\"\n",
    "    \n",
    "    \n",
    "    # returns support vectors with lambdas\n",
    "    def vectors(self):\n",
    "        return self.svX, self.l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решающее дерево"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все решающие деревья - бинарные деревья, каждой вершине $v$ которых соответствует подвыборки $R_v \\subset (X, Y)$. Для бинарной классификации примем $Y = \\{0, 1 \\}$\n",
    "\n",
    "Вершины делятся на:\n",
    "* Внутренние, содержащие предикаты типа $[f_j (x) < t]$, результат которых делит $R_v$ на две подвыборки $R_l$ и $R_r$, которые будут относиться к левой и правой дочерней вершине соответстенно.\n",
    "* Листы, в которых записан прогноз этого листа $c_v = \\underset{c}{\\operatorname{argmax}} \\sum\\limits^{|R_v|}_{i = 1} [y_i = c]$, где $(x_i, y_i) \\in R_v, \\; c \\in Y$.\n",
    "\n",
    "Деревья зачастую получаются переобученными, поэтому для повышения обобщающей способности пользуются алгоритом срижки деревьев *Cost Complexity Pruning*, который ищет $\\alpha$-оптимальное поддерево - то, у которого значение функционала $R_{\\alpha}(T) = R(T) + \\alpha |T|$ минимально. Мы будем задавать параметр $\\alpha$ в качестве параметра класса, а его оптимальный выбор оставим на совести программиста(можно подобрать на кросс-валидации). Заметим, справедливо: $R_{\\alpha}(T) = R_{\\alpha}(T_l) + R_{\\alpha}(T_r) = \\sum\\limits_{i = 1}^{|T|} R_{\\alpha}(t_{leaf})$, что позволит провести стрижку за один рекурсивный проход снизу вверх по дереву, поскольку минимизаци функционала $\\Delta R_{\\alpha}(T_{sub})$ для любого из поддеревьев $T_{sub}$ будет равна минимизации $\\Delta R_{\\alpha}(T)$ и для самого дерева $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лист дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем хранить во всех вершинах их прогноз и индексы элемнтов из общей выборки вне зависимости от того, листовая она или внутренняя, что упростит ускорит и упростит код, однако ведет к увеличению потребляемой памяти. В внутренних вершинах будет хранится функция - предикат, когда как у листа будет значение $None$.\n",
    "\n",
    "Этот класс нужен для хранения вершин, однако лишен автономности, поскольку управление вершинами не должно происходить в отрыве от дерева. Поэтому функции построения дочерних вершин, подбора параметров и стрижки будут произведены внутри дерева, которое будет иметь полный контроль над классом вершины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это реализация листа с хранением количества объектов разных классов для ускоренной реализации дерева:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNode:\n",
    "    def __init__(self, idxs=None, pos=None, neg=None, c=None):\n",
    "        self.predicat = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.positives = pos\n",
    "        self.negatives = neg\n",
    "        self.c = c\n",
    "        self.idxs = idxs\n",
    "\n",
    "    # setters:\n",
    "    def set_left(self, left_node):\n",
    "        self.left = left_node\n",
    "\n",
    "    def set_idxs(self, idxs):\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def set_right(self, right_node):\n",
    "        self.right = right_node\n",
    "\n",
    "    def set_predicat(self, predicat):\n",
    "        self.predicat = predicat\n",
    "\n",
    "    def set_class(self, c):\n",
    "        self.c = c\n",
    "        \n",
    "    def set_positives(self, positives):\n",
    "        self.positives = positives\n",
    "        \n",
    "    def set_negatives(self, negatives):\n",
    "        self.negatives = negatives\n",
    "\n",
    "    #getters:\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "\n",
    "    def get_class(self):\n",
    "        return self.c\n",
    "\n",
    "    def get_idxs(self):\n",
    "        return self.idxs\n",
    "    \n",
    "    def get_positives(self):\n",
    "        return self.positives\n",
    "        \n",
    "    def get_negatives(self):\n",
    "        return self.negatives\n",
    "    \n",
    "    def get_len(self):\n",
    "        return self.idxs.shape[0]\n",
    "\n",
    "    #checkers:\n",
    "    def is_leaf(self):\n",
    "        return self.predicat is None\n",
    "\n",
    "    def is_inner(self):\n",
    "        return not self.is_leaf()\n",
    "    \n",
    "    def make_leaf(self):\n",
    "        self.predicat = None\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решающее дерево"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Критерии информативности:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это новая реализация, которая работает за константное время, если в качестве аргументов предать количество положительных и отрицательных элементов в наборе вместо самого набора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bingini(*args):\n",
    "    # for binary classification: p(-) = 1 - p(+)=>\n",
    "    # Gini = 2 * p(+) * (1 - p(+))\n",
    "    if len(args) == 2:\n",
    "        p = args[0] / (args[1]+args[0])\n",
    "    else:\n",
    "        p = args[0].sum() / args[0].shape[0]\n",
    "    return 2 * p * (1 - p)\n",
    "\n",
    "def binentropy(*args):\n",
    "    # for binary classification: p(-) = 1 - p(+)=>\n",
    "    # Entropy = -p(+)log(p(+)) - (1 - p(+))log(1 - p(+))\n",
    "    if len(args) == 2:\n",
    "        p = args[0] / (args[1]+args[0])\n",
    "    else:\n",
    "        p = args[0].sum() / args[0].shape[0]\n",
    "    return -p*np.log(p) - (1 - p)*np.log(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация решающего дерева с возможностью наращивания дерева с помомщью метода случайных подпространств, что нужно для использования класса в качестве базового при строительстве *Случайного Леса*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:springgreen\">Внимание:</span> Здесь представлена ускоренная реализация дерева по отношению к старой реализации, котроую можно посмотреть ниже. Здесь ускоряется поиск разделения вершины по порогу и признаку дерева на 2 дочерних вершины. Это делается путем:\n",
    "- Выделения условно категориальных признаков, что позволяет искать разделение просто перебрав возможные значения.\n",
    "- Для условно непрерывных признаков производится предваврительная сортировка, что дает возможность вычисление критерия информативности и критерия качества за константное время во время прохода по значениям столбца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDescisionTree:\n",
    "    def __init__(self, criteria=bingini, pruning_cost=None, min_samples_split=2, random_sub_num=None):\n",
    "        self.CATEGORICAL_LEN = 10\n",
    "        self.is_categorical = None\n",
    "        self.categorical_vals = {}\n",
    "        self.H = criteria\n",
    "        self.root = None\n",
    "        self.min_split = min_samples_split\n",
    "        if random_sub_num is None:\n",
    "            self.random_subspace = False\n",
    "        else:\n",
    "            self.random_subspace = True\n",
    "            self.feature_num = random_sub_num\n",
    "        if pruning_cost is None:\n",
    "            self.pruning = False\n",
    "        else:\n",
    "            self.pruning = True\n",
    "            self.alpha = pruning_cost\n",
    "            \n",
    "    def node_classify(self, node):\n",
    "        # set class num node:\n",
    "        positives = node.get_positives()\n",
    "        negatives = node.get_negatives()\n",
    "        if positives >= negatives:\n",
    "            node.set_class(1)\n",
    "        else:\n",
    "            node.set_class(0)\n",
    "            \n",
    "    # create root and recursive building tree\n",
    "    def build_tree(self):\n",
    "        # create root with all nums\n",
    "        self.root = BinaryNode(np.arange(self.Y.shape[0]))\n",
    "        self.root.set_positives(self.Y.sum())\n",
    "        negatives = self.root.get_len() - self.root.get_positives()\n",
    "        self.root.set_negatives(negatives)\n",
    "        # recursive function of creation\n",
    "        self.recursive_creation(self.root)\n",
    "            \n",
    "            \n",
    "    # may be modifed\n",
    "    def stop_criteria(self, node):\n",
    "        # if num of eelems in node less than min required for split => 1\n",
    "        if node.get_len() < self.min_split:\n",
    "            return True\n",
    "        # if all elems in node has only one class => 1\n",
    "        positives = node.get_positives()\n",
    "        negatives = node.get_negatives()\n",
    "        return (negatives==0 or positives==0)\n",
    "    \n",
    "    # union of search_best_split() and split_node()\n",
    "    def search_best_split(self, node):\n",
    "        X_iter = self.X[node.get_idxs()]\n",
    "        Y_iter = self.Y[node.get_idxs()]\n",
    "        # compute node info criteria\n",
    "        positiv = node.get_positives()\n",
    "        negativ = node.get_negatives()\n",
    "        node_info = self.H(positiv, negativ)\n",
    "        # best params\n",
    "        best_gain = 0.0\n",
    "        best_j, best_t = 0, 0.0\n",
    "        # search in all features:\n",
    "        if self.random_subspace:\n",
    "            # get random permutation\n",
    "            features = np.random.permutation(self.X.shape[1])\n",
    "            # stay only feature_num random features\n",
    "            features = features[:self.feature_num]\n",
    "        # else search by all features\n",
    "        else:\n",
    "            features = range(self.X.shape[1])\n",
    "\n",
    "        for j in features:\n",
    "            column = X_iter[:, j]\n",
    "            # fast search if categorical:\n",
    "            if self.is_categorical[j]:\n",
    "                possible_vals = self.categorical_vals[j]\n",
    "                for i in range(1, possible_vals.shape[0]):\n",
    "                    mask = column < possible_vals[i]\n",
    "                    Y_r = Y_iter[mask]\n",
    "                    if Y_r.shape[0] == 0 or Y_r.shape[0] == Y_iter.shape[0]:\n",
    "                        continue\n",
    "                    right_pos = Y_r.sum()\n",
    "                    right_neg = Y_r.shape[0] - right_pos\n",
    "                    right_gini = self.H(right_pos, right_neg)\n",
    "                    left_gini = self.H(positiv - right_pos, negativ - right_neg)\n",
    "                    # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
    "                    gain = node_info\n",
    "                    gain -= (Y_r.shape[0]*right_gini/node.get_len())\n",
    "                    gain -= (1 - Y_r.shape[0]/node.get_len())*left_gini\n",
    "                    if gain > best_gain:\n",
    "                        best_t = possible_vals[i]\n",
    "                        best_j = j\n",
    "                        best_gain = gain  \n",
    "                continue\n",
    "            # else standart search:\n",
    "            sorted_col = np.argsort(column)\n",
    "            right_neg = 0\n",
    "            right_pos = 0\n",
    "            last_t = column[sorted_col[0]]\n",
    "            for i in range(1, column.shape[0]):\n",
    "                if Y_iter[sorted_col[i-1]]:\n",
    "                    right_pos += 1\n",
    "                else:\n",
    "                    right_neg += 1\n",
    "                    \n",
    "                idx = sorted_col[i]\n",
    "                if column[idx] == last_t:\n",
    "                    continue\n",
    "                \n",
    "                last_t = column[idx]\n",
    "                # compute gain:\n",
    "                right_gini = self.H(right_pos, right_neg)\n",
    "                left_gini = self.H(positiv - right_pos, negativ - right_neg)\n",
    "                # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
    "                gain = node_info\n",
    "                gain -= (i*right_gini/node.get_len()) + (1 - i/node.get_len())*left_gini\n",
    "                # needs best gain split\n",
    "                if gain > best_gain:\n",
    "                    best_t = column[idx]\n",
    "                    best_j = j\n",
    "                    best_gain = gain\n",
    "                    \n",
    "        if best_gain > 0.0:\n",
    "            return best_j, best_t\n",
    "    \n",
    "    # create 2 new nodes: left and right\n",
    "    def split_node(self, node, j, t):\n",
    "        # set predicat rule for node\n",
    "        predicat = lambda x: x[j] < t\n",
    "        node.set_predicat(predicat)\n",
    "        # get split mask\n",
    "        column = self.X[node.get_idxs(), j]\n",
    "        mask = column < t\n",
    "        # make idxs for left and right\n",
    "        right_idxs = node.get_idxs()[mask]\n",
    "        left_idxs = node.get_idxs()[np.logical_not(mask)]\n",
    "        #compute positives:\n",
    "        right_pos = self.Y[right_idxs].sum()\n",
    "        left_pos = self.Y[left_idxs].sum()\n",
    "        # compute negatives\n",
    "        right_neg = right_idxs.shape[0] - right_pos\n",
    "        left_neg = left_idxs.shape[0] - left_pos\n",
    "        # create nodes\n",
    "        node.set_left(BinaryNode(left_idxs, left_pos, left_neg))\n",
    "        node.set_right(BinaryNode(right_idxs, right_pos, right_neg))\n",
    "    \n",
    "    # recursive function for nodes:\n",
    "    def recursive_creation(self, node):\n",
    "        # classify another node:\n",
    "        self.node_classify(node)\n",
    "        # stop criteria for building\n",
    "        if self.stop_criteria(node):\n",
    "            return\n",
    "        #else find best split\n",
    "        jt = self.search_best_split(node)\n",
    "        # if we cant find best split - stop\n",
    "        if jt is None:\n",
    "            return\n",
    "\n",
    "        # split node for 2 child:\n",
    "        self.split_node(node, *jt)\n",
    "        # start recursion for left:\n",
    "        self.recursive_creation(node.get_left())\n",
    "        # for right:\n",
    "        self.recursive_creation(node.get_right())\n",
    "    \n",
    "    def tree_pruning(self, node):\n",
    "        # this node R_a(t) computing:\n",
    "        # R = sum([y != c]) / |N|\n",
    "        if node.get_class():\n",
    "            R_a = node.get_negatives() / node.get_len()\n",
    "        else:\n",
    "            R_a = node.get_positives() / node.get_len()\n",
    "        # R_a(t) = R(t) + a\n",
    "        R_a += self.alpha\n",
    "        # at first go while not leaf:\n",
    "        if node.is_leaf():\n",
    "            # return R_a(leaf)\n",
    "            return R_a\n",
    "        # R_a(Tl) and R_a(Tr)\n",
    "        R_al = self.tree_pruning(node.get_left())\n",
    "        R_ar = self.tree_pruning(node.get_right())\n",
    "        # if R_a(t) < R_a(T) => pruning\n",
    "        if R_a <= R_al + R_ar:\n",
    "            node.make_leaf()\n",
    "            return R_a\n",
    "        # else do nothing\n",
    "        return R_al + R_ar\n",
    "    \n",
    "    def search_categorical(self):\n",
    "        self.is_categorical = np.zeros(self.X.shape[1]).astype(np.int8)\n",
    "        for j in range(self.X.shape[1]):\n",
    "            uniq = np.unique(self.X[:, j])\n",
    "            if uniq.shape[0] < self.CATEGORICAL_LEN:\n",
    "                self.categorical_vals[j] = uniq\n",
    "                self.is_categorical[j] = 1\n",
    "            \n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # temp sets for comfort\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        # we will store only idxs while training in nodes =>\n",
    "        # make idxs column for comfort:\n",
    "        self.idxs = np.arange(X_train.shape[0])\n",
    "        # for speed search categorical\n",
    "        self.search_categorical()\n",
    "        # build tree \n",
    "        self.build_tree()\n",
    "        # pruning tree\n",
    "        if self.pruning:\n",
    "            self.tree_pruning(self.root)\n",
    "        #  delete temp sets:\n",
    "        #self.free_memory(root)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        del self.idxs\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
    "        # for each elem in X predict result:\n",
    "        for i in np.arange(X_val.shape[0]):\n",
    "            Y_pred[i] = self.predict_one(X_val[i])\n",
    "        return Y_pred\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        node = self.root\n",
    "        while node.is_inner():\n",
    "            if node.predicat(x):\n",
    "                node = node.get_right()\n",
    "            else:\n",
    "                node = node.get_left()\n",
    "        # if in leaf:\n",
    "        return node.get_class()\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Decision Tree\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Tree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно стоит в будущем добавить визуализацию дерева, поскольку интерпретируемость - важное свойство дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Старая реализаци"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Внимание:</span> Здесь показана моя старая реализация дерева, которая сильно медленне модификации, представленной выше. Её можно запустить и скорей всего получить корректный ответ, но на больших данных она показывает чудовищно большое время работы.\n",
    "\n",
    "Её можно удалить от сюда, для сокращения количества содержимого. Далее она не используется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldBinaryNode:\n",
    "    def __init__(self, idxs = None, c = None):\n",
    "        self.predicat = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.positives = None\n",
    "        self.negatives = None\n",
    "        if c is not None:\n",
    "            self.c = c\n",
    "        if idxs is not None:\n",
    "            self.idxs = idxs\n",
    "\n",
    "    # setters:\n",
    "    def set_left(self, left_node):\n",
    "        self.left = left_node\n",
    "\n",
    "    def set_idxs(self, idxs):\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def set_right(self, right_node):\n",
    "        self.right = right_node\n",
    "\n",
    "    def set_predicat(self, predicat):\n",
    "        self.predicat = predicat\n",
    "\n",
    "    def set_class(self, c):\n",
    "        self.c = c\n",
    "\n",
    "    #getters:\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "\n",
    "    def get_class(self):\n",
    "        return self.c\n",
    "\n",
    "    def get_idxs(self):\n",
    "        return self.idxs\n",
    "\n",
    "    #checkers:\n",
    "    def is_leaf(self):\n",
    "        return self.predicat is None\n",
    "\n",
    "    def is_inner(self):\n",
    "        return not self.is_leaf()\n",
    "    \n",
    "    def make_leaf(self):\n",
    "        self.predicat = None\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oldbingini(Y):\n",
    "    # for binary classification: p(-) = 1 - p(+)=>\n",
    "    # Gini = 2 * p(+) * (1 - p(+))\n",
    "    p = Y.sum() / len(Y)\n",
    "    return 2 * p * (1 - p)\n",
    "\n",
    "def oldbinentropy(Y):\n",
    "    # for binary classification: p(-) = 1 - p(+)=>\n",
    "    # Entropy = -p(+)log(p(+)) - (1 - p(+))log(1 - p(+))\n",
    "    p = Y.sum() / len(Y)\n",
    "    return -p*np.log(p) - (1 - p)*np.log(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if without pruning => pruning_cost = None\n",
    "# if without random subspace building => rndm_sub_num - None\n",
    "class OldBinaryDescisionTree:\n",
    "    def __init__(self, criteria=oldbingini, pruning_cost=None, min_samples_split=2):\n",
    "        self.H = criteria\n",
    "        self.root = None\n",
    "        self.min_split = min_samples_split\n",
    "        if pruning_cost is None:\n",
    "            self.pruning = False\n",
    "        else:\n",
    "            self.pruning = True\n",
    "            self.alpha = pruning_cost\n",
    "\n",
    "\n",
    "    # functional Q for separation\n",
    "    def functional(self, idxs, j, t):\n",
    "        # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
    "        RmX = self.X[idxs]\n",
    "        RmY = self.Y[idxs]\n",
    "        # make split:\n",
    "        mask = RmX[:, j] < t\n",
    "        RrY = RmY[mask]\n",
    "        RlY = RmY[np.logical_not(mask)]\n",
    "\n",
    "        return self.H(RmY) - len(RrY)*self.H(RrY) / len(RmY) - len(RlY)*self.H(RlY) / len(RmY)\n",
    "        \n",
    "    # may be modifed\n",
    "    def stop_criteria(self, node):\n",
    "        Y = self.Y[node.get_idxs()]\n",
    "        # if num of eelems in node less than min required for split => 1\n",
    "        if len(Y) < self.min_split:\n",
    "            return True\n",
    "        # if all elems in node has only one class => 1\n",
    "        positives = Y.sum()\n",
    "        return positives == len(Y) or positives == 0\n",
    "\n",
    "\n",
    "    # create root and recursive building tree\n",
    "    def build_tree(self):\n",
    "        # create root with all nums\n",
    "        self.root = OldBinaryNode(np.arange(len(self.Y)))\n",
    "        self.recursive_creation(self.root)\n",
    "\n",
    "    # compute gains and get best varios:\n",
    "    def search_best_split(self, node):\n",
    "        idxs = node.get_idxs()\n",
    "        best_gain = 0.0\n",
    "        best_j, best_t = 0, 0.0\n",
    "        # itterate for all possible values\n",
    "        X_iter = self.X[idxs]\n",
    "        for j in range(self.X.shape[1]):\n",
    "            for t in X_iter[:, j]:\n",
    "                gain = self.functional(idxs, j, t)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_j = j\n",
    "                    best_t = t\n",
    "        if best_gain > 0.0:\n",
    "            return best_j, best_t\n",
    "\n",
    "    # create 2 new nodes: left and right\n",
    "    def split_node(self, node, j, t):\n",
    "        # set predicat rule for node\n",
    "        predicat = lambda x: x[j] < t\n",
    "        node.set_predicat(predicat)\n",
    "        # get split mask\n",
    "        column = self.X[node.get_idxs(), j]\n",
    "        mask = column < t\n",
    "        # make idxs for left and right\n",
    "        right_idxs = node.get_idxs()[mask]\n",
    "        left_idxs = node.get_idxs()[np.logical_not(mask)]\n",
    "        # create nodes\n",
    "        node.set_left(OldBinaryNode(left_idxs))\n",
    "        node.set_right(OldBinaryNode(right_idxs))\n",
    "\n",
    "\n",
    "    def node_classify(self, node):\n",
    "        # set class num node:\n",
    "        positives = self.Y[node.get_idxs()].sum()\n",
    "        if positives >= (len(node.get_idxs()) + 1) // 2:\n",
    "            node.set_class(1)\n",
    "        else:\n",
    "            node.set_class(0)\n",
    "\n",
    "\n",
    "    # recursive function for nodes:\n",
    "    def recursive_creation(self, node):\n",
    "        # classify another node:\n",
    "        self.node_classify(node)\n",
    "        # stop criteria for building\n",
    "        if self.stop_criteria(node):\n",
    "            return\n",
    "        #else find best split\n",
    "        jt = self.search_best_split(node)\n",
    "        # if we cant find best split - stop\n",
    "        if jt is None:\n",
    "            return\n",
    "        # split node for 2 child:\n",
    "        self.split_node(node, *jt)\n",
    "        # start recursion for left:\n",
    "        self.recursive_creation(node.get_left())\n",
    "        # for right:\n",
    "        self.recursive_creation(node.get_right())\n",
    "\n",
    "    def tree_pruning(self, node):\n",
    "        # this node R_a(t) computing:\n",
    "        # R = sum([y != c]) / |N|\n",
    "        R_a = (self.Y[node.get_idxs()] != node.get_class()).sum() / node.get_idxs().shape[0]\n",
    "        # R_a(t) = R(t) + a\n",
    "        R_a += self.alpha\n",
    "        # at first go while not leaf:\n",
    "        if node.is_leaf():\n",
    "            # return R_a(leaf)\n",
    "            return R_a\n",
    "        # R_a(Tl) and R_a(Tr)\n",
    "        R_al = self.tree_pruning(node.get_left())\n",
    "        R_ar = self.tree_pruning(node.get_right())\n",
    "        # if R_a(t) < R_a(T) => pruning\n",
    "        if R_a <= R_al + R_ar:\n",
    "            node.make_leaf()\n",
    "            return R_a\n",
    "        # else do nothing\n",
    "        return R_al + R_ar\n",
    "        \n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # temp sets for comfort\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        # we will store only idxs while training in nodes =>\n",
    "        # make idxs column for comfort:\n",
    "        self.idxs = np.arange(X_train.shape[0])\n",
    "        # build tree \n",
    "        self.build_tree()\n",
    "        # pruning tree\n",
    "        if self.pruning:\n",
    "            self.tree_pruning(self.root)\n",
    "        #  delete temp sets:\n",
    "        #self.free_memory(root)\n",
    "        del self.X\n",
    "        del self.Y\n",
    "        del self.idxs\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
    "        # for each elem in X predict result:\n",
    "        for i in np.arange(X_val.shape[0]):\n",
    "            Y_pred[i] = self.predict_one(X_val[i])\n",
    "        return Y_pred\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        node = self.root\n",
    "        while node.is_inner():\n",
    "            if node.predicat(x):\n",
    "                node = node.get_right()\n",
    "            else:\n",
    "                node = node.get_left()\n",
    "        # if in leaf:\n",
    "        return node.get_class()\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Decision Tree\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Tree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно удалить это реализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод k ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Метод k ближайших соседей* классифицирует объекты лениво, то есть не требует настройки параметров по обучающей выборке, а просто запоминает её и делает предсказания на основании классов ближайших соседей предсказываемого объекта. Для измерения близости необходимо задать метрику $\\rho(x, y)$.\n",
    "\n",
    "Для предсказания класса $c$ объекта $x$ необходимо найти в обучающей выборке такие $k$ объектов c наименьшим растоянием до объекта $\\Omega = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^{k+1}, \\; \\Omega \\subset (X, Y): \\; \\; \\rho(x, x^{(0)}) \\leq ... \\leq \\rho(x, x^{(i)}) \\leq ... \\leq \\rho(x, x^{(k)})$ и $\\rho(x, x_1) \\leq \\rho(x, x_2), \\; \\forall (x_1, y_1) \\in \\Omega, \\; \\forall (x_2, y_2) \\in (X, Y) \\setminus \\Omega$. Тогда класс объекта согласно *Методу окна Парзена переменной ширины* определяется по формуле:\n",
    "\n",
    "$$c = \\underset{y \\in Y}{\\operatorname{argmax}} \\sum\\limits^{k}_{i = 1} [y = y^{(i)}] K \\left( \\displaystyle\\frac{\\rho(x, x^{i})}{\\rho(x, x^{k+1})} \\right)$$\n",
    "\n",
    "Здесь $K(t)$ - функция(ядро), неубывающая на $t \\in [0, 1]$. При $K(t)  = 1 = const$ *Метод окна Парзена* становится *Методом k ближайших соседей.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-nearest neighbors algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможные метрики для алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkovski(x1, x2, p = 3):\n",
    "    return (np.abs(x1 - x2) ** p).T.sum(axis = 0)**(1.0 / p)\n",
    "\n",
    "def euclid(x1, x2):\n",
    "    return minkovski(x1, x2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможные невозрастающие ядра $K(t) \\in [0, 1], \\; t \\in [0, 1]$ для обобщения метода до *Метода окна Парзена:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biquadratical_kernel(t):\n",
    "    return (-t**2 + 1)**2\n",
    "\n",
    "def triquadratical_kernel(t):\n",
    "    return (-t**2 + 1)**3\n",
    "    \n",
    "def triqubical_kernel(t):\n",
    "    return (-t**3 + 1)**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация метода с возможностью учета расстояний между соседями, однако без отбора эталонных элементов и эвристик, ускоряющих поиск соседей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay parsen kerel None for simple KNN algo\n",
    "class BinaryKNN:\n",
    "    def __init__(self, k, metric = euclid, parsen_kernel = None):\n",
    "        self.k = k\n",
    "        self.p = metric\n",
    "        # algo shoud remember all data\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        if parsen_kernel is None:\n",
    "            # all elems have equal weight\n",
    "            self.Kp = lambda t: 1.0\n",
    "        else:\n",
    "            # parsen method\n",
    "            self.Kp = parsen_kernel\n",
    "\n",
    "    def stolp_filtration(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        # just remmember data:\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        # check correct of k\n",
    "        self.k = min(self.k, len(self.Y) - 1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        Y_pred = np.zeros(len(X_val)).astype(np.int8)\n",
    "        # for each elem in X predict result:\n",
    "        for i in np.arange(len(X_val)):\n",
    "            Y_pred[i] = self.predict_one(X_val[i])\n",
    "        return Y_pred\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        # compute all distances\n",
    "        r_x = self.p(self.X, x)\n",
    "        # take sorted order of dists in idxs\n",
    "        order = np.argsort(r_x)\n",
    "        # width for parsen is distance to k+1 neiborh:\n",
    "        h = r_x[order[self.k]]\n",
    "        # idxs of first k elems neibrh\n",
    "        order = order[:self.k]\n",
    "        # take first k Y:\n",
    "        Y_k = self.Y[order]\n",
    "        # compute parsen function for all neiborh:\n",
    "        K = self.Kp(r_x[order] / h)\n",
    "        \n",
    "        # compute functional for positives elems\n",
    "        pos_w = (K * Y_k).sum()\n",
    "        # compute functional for negatives elems\n",
    "        neg_w = (K * np.logical_not(Y_k)).sum()\n",
    "        \n",
    "        # class with more functional wins\n",
    "        return int(pos_w > neg_w) # 0 or 1\n",
    "\n",
    "\n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"k Nearest Neighbor\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"KNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В будущем можно будет добавить реализацию алгоритма **STOLP**, для классификации только по эталонным объектам. Было решено не добавлять, так как появится 1-2 новых параметра алгоритма, которые также будет необходимо подбирать, что усложнит работу с классом для людей незнакомыми со **STOLP**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод построения композиции классификаторов, называемый *Случайный Лес*, призван бороться с главным недостатком решающих деревьев - переобучением.\n",
    "\n",
    "Если дана обучающая выборка $(X, Y)$, алгоритм построения леса из $N$ деревьев выглядит так:\n",
    "\n",
    "Для каждого $k = 1 \\dots N$:\n",
    "\n",
    "1. Генерируем случайную подвыборку с повторениями $(X_k, Y_k)$\n",
    "2. Начинаем строить решающее дерево $b_k$ на выборке $(X_k, Y_k)$:\n",
    "   - На каждом сплите выбираем случайное подмножество признаков $d_l$\n",
    "   - Из этого подмножества $d_l$ берем тот признак, сплит по которому является наилучшим согласно критерию\n",
    "   - Строим дерево до тех пор, пока не достигается максимально указанная глубина _max_depth_ или в листьях не оказывается _n_min_ объектов\n",
    "   \n",
    "Получаем итоговый ансабмль моделей (лес деревьев) в виде голосования моделей:\n",
    "\n",
    "$$\\large a(x) = \\frac{1}{N}\\sum_{k = 1}^N b_k(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryRandomForest:\n",
    "    def __init__(self, Ntrees=20, criteria=bingini, random_sub_num=None):\n",
    "        self.N = Ntrees\n",
    "        self.criteria = criteria\n",
    "        self.sub_space_num = random_sub_num\n",
    "        # list for trained models\n",
    "        self.trees = [None for _ in range(self.N)]\n",
    "\n",
    "    # get random sample for tree training\n",
    "    def bootstrap_sample(self, X, Y):\n",
    "        # indexes of X and Y\n",
    "        indexes = np.arange(len(Y))\n",
    "        # rundom indexes with repeats\n",
    "        indexes = np.random.choice(indexes, len(indexes))\n",
    "        # bootstrap sample\n",
    "        return X[indexes], Y[indexes]\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        if self.sub_space_num is None:\n",
    "            self.sub_space_num = int(len(X_train)**(1/2))\n",
    "        # train N trees with \n",
    "        for i in range(self.N):\n",
    "            # create tree(use our class of tree)\n",
    "            self.trees[i] = BinaryDescisionTree(\n",
    "                self.criteria, # user criteria\n",
    "                None, # trees without prunning\n",
    "                2, # build while it possible \n",
    "                self.sub_space_num) # num of random features in random space method\n",
    "            # and train tree:\n",
    "            self.trees[i].fit(*self.bootstrap_sample(X_train, Y_train))\n",
    "        return self\n",
    "            \n",
    "\n",
    "    def predict(self, X_val):\n",
    "        voices = np.zeros(len(X_val))\n",
    "        # make vote:\n",
    "        for tree in self.trees:\n",
    "            voices += tree.predict(X_val)\n",
    "        # compute winners:\n",
    "        Y_pred = voices >= (self.N + 1) // 2\n",
    "        return Y_pred.astype(np.int8)\n",
    "\n",
    "    \n",
    "    def score(self, X_val, Y_val, metric=Accuracy):\n",
    "        return metric(Y_val, self.predict(X_val))\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Random Forest\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"RF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее не представлена демонстрация работы *Случайного леса*, поскольку у меня не было его по заданию, а ждать его обучения вместе с кросс-валидацией слишком долго. Однако его работоспособность обробованна на других датасетах. Можете попробовать и вы!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> Использование </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn реализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импортируем данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем уже обработанный в 1-ой лабораторной работе датасет, который заботливо разделён на тренировочный и валидационный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>mnth_6</th>\n",
       "      <th>mnth_7</th>\n",
       "      <th>mnth_8</th>\n",
       "      <th>mnth_9</th>\n",
       "      <th>mnth_10</th>\n",
       "      <th>mnth_11</th>\n",
       "      <th>mnth_12</th>\n",
       "      <th>dull</th>\n",
       "      <th>highP</th>\n",
       "      <th>fastwind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>9.35</td>\n",
       "      <td>44.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>9.35</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>9.35</td>\n",
       "      <td>46.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  WindGustSpeed  \\\n",
       "0     13.4     22.9       0.6          5.2      9.35           44.0   \n",
       "1      7.4     25.1       0.0          5.2      9.35           44.0   \n",
       "2     12.9     25.7       0.0          5.2      9.35           46.0   \n",
       "\n",
       "   WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  ...  mnth_6  mnth_7  \\\n",
       "0          20.0          24.0         71.0         22.0  ...       0       0   \n",
       "1           4.0          22.0         44.0         25.0  ...       0       0   \n",
       "2          19.0          26.0         38.0         30.0  ...       0       0   \n",
       "\n",
       "   mnth_8  mnth_9  mnth_10  mnth_11  mnth_12  dull  highP  fastwind  \n",
       "0       0       0        0        0        1     0      0         0  \n",
       "1       0       0        0        0        1     0      0         0  \n",
       "2       0       0        0        0        1     0      0         1  \n",
       "\n",
       "[3 rows x 133 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "test_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что датасет отсортирован по годам, поэтому при валидации стоит это учесть. Выделим столбец *Year* для удобства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные с 2007 по 2015 год.\n"
     ]
    }
   ],
   "source": [
    "year_validate = train_df['Year'].to_numpy()\n",
    "print(\"Данные с\", year_validate.min(), \"по\", year_validate.max(), \"год.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед обучением выделим целевую переменную и нормализуем данные, поскольку ради удобства мы сохранили датасет в файл до проведения этих операций. На выходе получим *numpy-array*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_df['RainTomorrow'].to_numpy()\n",
    "Y_test = test_df['RainTomorrow'].to_numpy()\n",
    "\n",
    "X_train = train_df.drop('RainTomorrow', axis = 1)\n",
    "X_test = test_df.drop('RainTomorrow', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_features_std = MinMaxScaler()\n",
    "\n",
    "X_train = scale_features_std.fit_transform(X_train.to_numpy()) \n",
    "X_test = scale_features_std.transform(X_test.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стратегия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попрообуем подобрать оптимальные параметры для моделей на тренеровочной выборке, после чего будем сравнивать качество полученных моделей на тестовой выборке!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вспомогательные инструменты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кросс валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подбора параметров будем использовать технику кросс-валидации!\n",
    "\n",
    "Так как мы хотим научить модели предсказывать данные из будущего, проведем кросс-валидацию моделей по следующей схеме: \n",
    "\n",
    "<center><img src=\"imgs/cv.png\" alt=\"CV\" title=\"Cross-Validation Time Serias\"/></center>\n",
    "\n",
    "Оптимизировать будем долю угаданных отвеов модели, поскольку нам важно как можно удачнее предсказывать *наличие/отсутствие* дождя на следующий день."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как наш датасет варъируется по годам с 2008-го по 2015-ый, будем  валидироваться на 2013, 2014 и 2015 году."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_cross_validation(model_from_param, param_generator):\n",
    "    max_val = 2015\n",
    "    min_val = 2013\n",
    "    best_score = 0.0\n",
    "    best_param = None\n",
    "    for p in tqdm(param_generator):\n",
    "        mean_score = 0.0\n",
    "        for year in range(min_val, max_val + 1):\n",
    "            mask_train = year_validate < year\n",
    "            mask_val = year_validate == year\n",
    "            X_t = X_train[mask_train]\n",
    "            Y_t = Y_train[mask_train]\n",
    "            X_v = X_train[mask_val]\n",
    "            Y_v = Y_train[mask_val]\n",
    "            model = model_from_param(p)\n",
    "            model.fit(X_t, Y_t)\n",
    "            mean_score += model.score(X_v, Y_v)\n",
    "        mean_score /= max_val - min_val + 1\n",
    "        if best_score < mean_score:\n",
    "            best_score = mean_score\n",
    "            best_param = p\n",
    "    return best_param, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашу логистическую регрессию с L2 регуляризацией. Будем настраивать параметр регуляризации $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [37:59<00:00, 227.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param is 0.8 with accuracy: 0.8415168310511146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.01, 0.1)\n",
    "modeling = lambda c: BinaryLogisticRegression(reg_type='l2', C=c, maxsteps=1500, speed=0.02)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best param is\", best_C, \"with accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Logistic Regression"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_log_l2_model = BinaryLogisticRegression(reg_type='l2', C=best_C, maxsteps=1500, speed=0.02)\n",
    "my_log_l2_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375595215051682"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_log_l2_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию из *sklearn* на L2 регляризацию аналогично."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:13<00:00,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param is 0.5 with accuracy: 0.8540712084758995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.01, 0.1)\n",
    "modeling = lambda c: LogisticRegression(penalty='l2', C=c, max_iter=250)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best param is\", best_C, \"with accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=250,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_log_l2_model = LogisticRegression(penalty='l2', C=best_C, max_iter=250)\n",
    "skl_log_l2_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424760946149975"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_log_l2_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моя реализация уступает в качестве, поскольку мой решатель более груб, нежели чем из библиотеки, использующий приближенные к *ньютоновским* методы спуска. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для экономии времени я не стал тестировать модели с *L1* регуляризацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решающее дерево"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим моё решающее дерево, продолжая строительство, пока это возможно. Будем использовать *стрижку* дерева для борьбы с переобучением. Настраивать $\\alpha$ мы на валидации не совсем корректно, поскольку он чувствителен к количеству данных, а в нашем варианте валидации алгоритм получает разное колиество данных для обучения, поэтому сделаем выбор параметра простым подбором значений вручную."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также настройка займет много времени при том, что одно дерево в общем классифицирует хуже других классификаторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_a = 0.000019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_desc_tree_model = BinaryDescisionTree(pruning_cost=best_a)\n",
    "my_desc_tree_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8198676009446014"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_desc_tree_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим решающее дерево из *sklearn* аналогично."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_a = 0.00008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=8e-05, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_desc_tree_model = DecisionTreeClassifier(ccp_alpha=best_a)\n",
    "skl_desc_tree_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8321396771321281"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_desc_tree_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если не проводить прунинг, мое дерево на несколько тысячных лучше импортированного(этот пример я не стал сохранять), однако, подобрать параметры стрижки моего дерево оказалось намного труднее, что и дало преимущество сторонней реализации перед моей. С причиной этого я до конца не разобрался, но подозреваю, что мое дерево имеет большее количество литовых листов в силу некоторых различий в реализации дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод k ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим *KNN* из *sklearn* на метрике *Евклида*, с настройкой количества соседей $k$ на валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:52:15<00:00, 1347.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param is 31 with accuracy: 0.8207719809612377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_k = np.arange(1, 65, 15)\n",
    "modeling = lambda K: KNeighborsClassifier(n_neighbors=K)\n",
    "best_k, best_score = time_cross_validation(modeling, generate_k)\n",
    "print(\"Best param is\", best_k, \"with accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=31, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "skl_knn_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803066083388177"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_knn_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим мою реализации классического *KNN* алгоритма аналогично, но наилучшее значение $k$ возьмем из полученного на валидации, поскольку моя реализация медленнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNN"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_knn_model = BinaryKNN(k=best_k)\n",
    "my_knn_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803066083388177"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_knn_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы работают идентично в силу однозначности и прямолинейности используемого алгоритма. И моя, и сторонняя реализации работают медленно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В силу того, что обучение слишком долгое, обобщение в виде *метода окна Парзена* здесь тестировать не будем, но ожидается он должен работать лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим *sklearn* реализацию *SVM* алгоритма с линейным ядром, с настройкой параметра $С$ L2 реализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:11<00:00, 31.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param is 1.0 with accuracy: 0.8547411203312341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_c = np.arange(0.1, 1.1, 0.1)\n",
    "modeling = lambda c: LinearSVC(C=c)\n",
    "best_C, best_score = time_cross_validation(modeling, generate_c)\n",
    "print(\"Best param is\", best_C, \"with accuracy:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_svm_model = LinearSVC(C=best_C)\n",
    "skl_svm_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424373814409043"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_svm_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим мой линейный *SVM* с лучшим параметром регуляризации, полученном для *sklearn* реализации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVM"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm_model = BinarySVM(kernel=None, C=best_C, maxsteps=1000)\n",
    "my_svm_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421663892222523"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моя реализация по качеству очень близка к *sklearn LinearSVC* в силу того, что обучение производться не градиентным спуском, как в случае *Логистичесой регрессии*, а *SMO алгоритмом*, что лишает необходимости настраивать количество шагов и их длину при обучении. Моя реализация оказалась очень медленной, однако стандартный *SVC*, который я пробовал обучить, также исполнялся огромное количество времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы не попробовали другие ядра для метода, такие как *RBF*, поскольку обучение на нашем наборе данных занимает слишком много времени. Стоит упомянуть, что нелинейная реализация *sklearn* работает также крайне долго на этом наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> Итог <h2><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой лабораторной работе я реализовал несколко основных алгоритма машинного обучения и сделал с их помощью предсказания. Эта работа мне понравилась, поскольку она напрямую связанна с первой  лабораторной работой, что дает ощущение последовательности обучения. Я потратил много время на написание кода, поскольку мне пришлось досконально разобраться с алгоритмами, которые я реализовывал, что показалось мне полезным и я не жалею о потраченном на это время. \n",
    "\n",
    "Из трудностей хотелось бы отметить отсутствие литературы на русско языке по многим темам, которые мне понадобилось изучить во время выполнения работы, а также посетовать на то, что скорость моего железа на ноутбуке не позволяет быстро обучать модели, из-за чего я даже не опробовал некоторые фишки реализованных алгоритмов, такие как обобщение *KNN* до *метода окна Парзена*, качество классификации которого должно быть сильно лучше, чем у стандартного алгоритма.\n",
    "\n",
    "Из минусов своей работы могу  выделить то, что выбранная мной метрика *Accuracy* является не совсем удачной для данного датасета, поскольку в этой задаче даже если классификатор будет возвращать $0$ для всех объектов, он получит значение метрики приблизительно $78\\%$. Также на лицо то, что написанные мной алгоритмы заметно уступают в скорости реализациям из *sklearn*, однако стоит отметить, что эта разница такова во многом из-за разности в производительности языка *Python* и *C*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <center>Сделано Бронниковым Максимом</center>\n",
    "###### <center>07.06.2020</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
